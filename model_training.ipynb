{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jullazarovych/DL_math_misunderstandings/blob/main/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "haIuT2gspJSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "033OkiH9SLsn"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers[torch] peft accelerate bitsandbytes scikit-learn pandas sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOe6gCWCQ3Z-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from torch import nn\n",
        "from peft import PeftModel\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import re\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset as TorchDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "import gc\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, balanced_accuracy_score, matthews_corrcoef\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36FbuBHzicSq"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Rg0b3L3iVC"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/MyDrive/nlp_math_misunderstanding/weights\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbqcMp2s53CS"
      },
      "outputs": [],
      "source": [
        "data_llm = \"/content/drive/MyDrive/nlp_math_misunderstanding/data/processed/train_v1.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erL5c26BvRts"
      },
      "outputs": [],
      "source": [
        "data_no_llm = \"/content/drive/MyDrive/nlp_math_misunderstanding/data/processed/train_v2.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v62-B22S5VJu"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_no_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqa_oC_o5oZE"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['Combined', 'Misconception'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvCoCWtZfTmx"
      },
      "outputs": [],
      "source": [
        "X_text = df['Combined'].astype(str).tolist()\n",
        "y_labels = df['Misconception'].tolist()\n",
        "\n",
        "X_train_text, X_val_text, y_train_labels, y_val_labels = train_test_split(\n",
        "    X_text, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
        ")\n",
        "\n",
        "print(f\"len of train: {len(X_train_text)}\")\n",
        "print(f\"len of test: {len(X_val_text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnPRzTzlSPBt"
      },
      "source": [
        "\n",
        "# **baseline model**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHvFvqI7Q4r_"
      },
      "outputs": [],
      "source": [
        "baseline_model = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),\n",
        "    ('clf', LogisticRegression(max_iter=1000, C=1.0))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SegCAgdeOqf"
      },
      "outputs": [],
      "source": [
        "baseline_model.fit(X_train_text, y_train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXy2Gw9XQ4oo"
      },
      "outputs": [],
      "source": [
        "preds = baseline_model.predict(X_val_text)\n",
        "print(f\"Baseline Accuracy: {accuracy_score(y_val_labels, preds)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_o00iiUSSRP"
      },
      "source": [
        "# **DNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I1z41EcTAAB"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['Misconception'])\n",
        "num_labels = len(le.classes_)\n",
        "id2label = {i: label for i, label in enumerate(le.classes_)}\n",
        "label2id = {label: i for i, label in enumerate(le.classes_)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTho-t63Q4kJ"
      },
      "outputs": [],
      "source": [
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtTYorz6Q4fs"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"Combined\"], truncation=True, max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UA1Ol4hQ4bi"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUZKXebgQ4XX"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"query\", \"value\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oenOeEsDQ4NK"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir='./logs',\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    label_smoothing_factor=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQWzDGL4TkzZ"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53gcIzh_URsf"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "\n",
        "    precision_macro = precision_score(labels, predictions, average='macro', zero_division=0)\n",
        "    recall_macro = recall_score(labels, predictions, average='macro', zero_division=0)\n",
        "    f1_macro = f1_score(labels, predictions, average='macro')\n",
        "\n",
        "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
        "\n",
        "    balanced_acc = balanced_accuracy_score(labels, predictions)\n",
        "\n",
        "    mcc = matthews_corrcoef(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"f1_weighted\": f1_weighted,\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"balanced_accuracy\": balanced_acc,\n",
        "        \"mcc\": mcc\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV3VtDDjQ0fm"
      },
      "outputs": [],
      "source": [
        "class TextDataset(TorchDataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GclEqKHMX9ew"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model_checkpoint = \"roberta-base\"\n",
        "\n",
        "texts = df['Combined'].tolist()\n",
        "labels = df['label'].tolist()\n",
        "num_labels = len(np.unique(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEkfMdHmn9g-"
      },
      "outputs": [],
      "source": [
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JI_ldxuX-5P"
      },
      "outputs": [],
      "source": [
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "last_fold_preds_ids = None\n",
        "last_fold_true_labels = None\n",
        "print(f\"Starting {n_splits}-Fold Cross-Validation...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train_cv, df_holdout = train_test_split(\n",
        "    df,\n",
        "    test_size=300,\n",
        "    random_state=42,\n",
        "    stratify=df['label']\n",
        ")\n",
        "\n",
        "print(f\"Full Train size: {len(df_train_cv)}\")\n",
        "print(f\"Holdout Test size: {len(df_holdout)}\")\n",
        "\n",
        "df_holdout.to_csv(f\"{output_dir}/holdout_test_300.csv\", index=False)\n",
        "print(\"Holdout dataset saved to drive.\")\n",
        "\n",
        "texts = df_train_cv['Combined'].tolist()\n",
        "labels = df_train_cv['label'].tolist()\n",
        "\n",
        "df_train_cv = df_train_cv.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "hHEsqdY8r6Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjtLBQRiL7S5"
      },
      "outputs": [],
      "source": [
        "all_fold_histories = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
        "    print(f\"\\n--- Fold {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    train_texts_fold = [texts[i] for i in train_idx]\n",
        "    val_texts_fold = [texts[i] for i in val_idx]\n",
        "    train_labels_fold = [labels[i] for i in train_idx]\n",
        "    val_labels_fold = [labels[i] for i in val_idx]\n",
        "\n",
        "    classes_in_fold = np.unique(train_labels_fold)\n",
        "    fold_weights = compute_class_weight('balanced', classes=classes_in_fold, y=train_labels_fold)\n",
        "    weights_full = np.ones(num_labels)\n",
        "    for cls, weight in zip(classes_in_fold, fold_weights):\n",
        "        weights_full[cls] = weight\n",
        "    weights_tensor = torch.tensor(weights_full, dtype=torch.float).to(device)\n",
        "\n",
        "    train_encodings = tokenizer(train_texts_fold, truncation=True, padding=True, max_length=256)\n",
        "    val_encodings = tokenizer(val_texts_fold, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "    train_dataset = TextDataset(train_encodings, train_labels_fold)\n",
        "    val_dataset = TextDataset(val_encodings, val_labels_fold)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    model.to(device)\n",
        "\n",
        "    trainer = WeightedLossTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    history = trainer.state.log_history\n",
        "    for entry in history:\n",
        "        entry['fold'] = fold + 1\n",
        "    all_fold_histories.extend(history)\n",
        "    eval_result = trainer.evaluate()\n",
        "    print(f\"Fold {fold+1} Result: Accuracy: {eval_result['eval_accuracy']:.4f}, F1-Macro: {eval_result['eval_f1_macro']:.4f}\")\n",
        "    fold_results.append(eval_result)\n",
        "\n",
        "    fold_save_path = f\"{output_dir}/fold_{fold+1}\"\n",
        "    trainer.save_model(fold_save_path)\n",
        "    print(f\"Model for Fold {fold+1} saved to Drive.\")\n",
        "\n",
        "    with open(f\"{output_dir}/all_results.json\", 'w') as f:\n",
        "        json.dump(fold_results, f)\n",
        "\n",
        "    if fold == n_splits - 1:\n",
        "        print(\"Generating predictions for Confusion Matrix...\")\n",
        "        predictions = trainer.predict(val_dataset)\n",
        "        if isinstance(predictions.predictions, tuple):\n",
        "            logits = predictions.predictions[0]\n",
        "        else:\n",
        "            logits = predictions.predictions\n",
        "        last_fold_preds_ids = np.argmax(logits, axis=1)\n",
        "        last_fold_true_labels = predictions.label_ids\n",
        "\n",
        "    del model, trainer, train_dataset, val_dataset\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TEST_FILE = f\"{output_dir}/holdout_test_300.csv\"\n",
        "FOLD_TO_TEST = 1\n",
        "MODEL_PATH = f\"{output_dir}/fold_{FOLD_TO_TEST}\"\n",
        "\n",
        "print(f\"Loading Holdout Test Data from: {TEST_FILE}\")\n",
        "df_test = pd.read_csv(TEST_FILE)\n",
        "\n",
        "test_texts = df_test['Combined'].tolist()\n",
        "test_labels = df_test['label'].tolist()\n",
        "\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
        "\n",
        "test_dataset = TextDataset(test_encodings, test_labels)\n",
        "\n",
        "print(f\"\\n--- Loading Model from Fold {FOLD_TO_TEST} ---\")\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "inference_model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
        "inference_model.to(device)\n",
        "inference_model.eval()\n",
        "\n",
        "print(\"Running prediction...\")\n",
        "tester = Trainer(model=inference_model)\n",
        "preds_output = tester.predict(test_dataset)\n",
        "\n",
        "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
        "y_true = test_labels\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"FINAL HOLDOUT RESULT (Fold {FOLD_TO_TEST})\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"{'='*30}\\n\")\n",
        "all_label_ids = sorted(label2id.values())\n",
        "target_names = [k for k, v in sorted(label2id.items(), key=lambda item: item[1])]\n",
        "\n",
        "print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=all_label_ids,\n",
        "    target_names=target_names,\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "print(\"\\n--- error examples ---\")\n",
        "df_test['pred'] = y_pred\n",
        "errors = df_test[df_test['label'] != df_test['pred']].head(5)\n",
        "for i, row in errors.iterrows():\n",
        "    true_label_name = id2label.get(row['label'], \"Unknown\")\n",
        "    pred_label_name = id2label.get(row['pred'], \"Unknown\")\n",
        "\n",
        "    print(f\"\\nText: {str(row['Combined'])[:100]}...\")\n",
        "    print(f\"True: {true_label_name} | Pred: {pred_label_name}\")"
      ],
      "metadata": {
        "id": "2mwnck10sF3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def visualize_classification_results(y_true, y_pred, labels, target_names):\n",
        "    report_dict = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        labels=labels,\n",
        "        target_names=target_names,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    df_report = pd.DataFrame(report_dict).transpose()\n",
        "    classes_df = df_report.drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore')\n",
        "    classes_df = classes_df.sort_values(by='f1-score', ascending=True)\n",
        "\n",
        "    plt.figure(figsize=(10, len(classes_df) * 0.4))\n",
        "    sns.heatmap(\n",
        "        classes_df[['precision', 'recall', 'f1-score']],\n",
        "        annot=True,\n",
        "        cmap='RdYlGn',\n",
        "        fmt='.2f',\n",
        "        linewidths=.5,\n",
        "        vmin=0, vmax=1\n",
        "    )\n",
        "    plt.title('Detailed metrics by class (Heatmap)', fontsize=15)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plot_df = classes_df[classes_df['support'] > 0]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, len(plot_df) * 0.45))\n",
        "\n",
        "    norm = plt.Normalize(0, 1)\n",
        "    colors = plt.cm.viridis(norm(plot_df['f1-score'].values))\n",
        "\n",
        "    bars = ax.barh(plot_df.index, plot_df['support'], color=colors)\n",
        "\n",
        "    sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = plt.colorbar(sm, ax=ax)\n",
        "    cbar.set_label('F1-Score', rotation=270, labelpad=15)\n",
        "\n",
        "    ax.set_xlabel('Number of examples in the test (support)')\n",
        "    ax.set_title('Class distribution and their quality (Color = F1)', fontsize=15)\n",
        "\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
        "                f'{int(width)}', va='center', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MNKjzDyTw5GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_label_ids = sorted(label2id.values())\n",
        "target_names = [k for k, v in sorted(label2id.items(), key=lambda item: item[1])]\n",
        "\n",
        "visualize_classification_results(y_true, y_pred, all_label_ids, target_names)"
      ],
      "metadata": {
        "id": "nbELOJDlxZa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_history = pd.DataFrame(all_fold_histories)\n",
        "train_loss = df_history.dropna(subset=['loss'])\n",
        "\n",
        "val_metrics = df_history.dropna(subset=['eval_loss'])\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.lineplot(data=train_loss, x='epoch', y='loss', label='Train Loss', color='blue')\n",
        "sns.lineplot(data=val_metrics, x='epoch', y='eval_loss', label='Val Loss', color='red')\n",
        "plt.title('Training & Validation Loss (Average over Folds)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "if 'eval_f1_macro' in val_metrics.columns:\n",
        "    sns.lineplot(data=val_metrics, x='epoch', y='eval_f1_macro', label='Val F1 Macro', marker='o', color='orange')\n",
        "\n",
        "if 'eval_accuracy' in val_metrics.columns:\n",
        "    sns.lineplot(data=val_metrics, x='epoch', y='eval_accuracy', label='Val Accuracy', linestyle='--', color='green')\n",
        "\n",
        "plt.title('Validation Metrics (Average over Folds)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c5l-mD1dndKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVED_MODEL_PATH = \"/content/drive/My Drive/nlp_math_misunderstanding/weights/fold_2\"\n",
        "MODEL_CHECKPOINT = \"roberta-base\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Loading base model...\")\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=33,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"Loading LoRA adapters from {SAVED_MODEL_PATH}...\")\n",
        "model = PeftModel.from_pretrained(base_model, SAVED_MODEL_PATH)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "text = \"The answer is 5/8 because I added top and bottom.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class_id = logits.argmax().item()\n",
        "\n",
        "print(f\"Test Prediction Class ID: {predicted_class_id}\")"
      ],
      "metadata": {
        "id": "buPdIpewAWZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "pI3zYh5WBOFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSR59KKudzeL"
      },
      "outputs": [],
      "source": [
        "with open(f\"{output_dir}/all_results.json\", 'w') as f:\n",
        "    json.dump(fold_results, f)\n",
        "\n",
        "avg_acc = np.mean([res['eval_accuracy'] for res in fold_results])\n",
        "avg_f1 = np.mean([res['eval_f1_macro'] for res in fold_results])\n",
        "print(f\"\\n=== Final Results: Avg Acc: {avg_acc:.4f}, Avg F1: {avg_f1:.4f} ===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4sI4DnnT8n5"
      },
      "outputs": [],
      "source": [
        "if last_fold_preds_ids is not None:\n",
        "    print(\"Building Confusion Matrix for the last fold...\")\n",
        "\n",
        "    cm = confusion_matrix(last_fold_true_labels, last_fold_preds_ids)\n",
        "\n",
        "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    sns.heatmap(cmn, annot=True, fmt='.1f',\n",
        "                xticklabels=list(label2id.keys()),\n",
        "                yticklabels=list(label2id.keys()),\n",
        "                cmap='Blues')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title('Normalized Confusion Matrix (Last Fold)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "    plt.savefig(f\"{output_dir}/confusion_matrix.png\")\n",
        "else:\n",
        "    print(\"Error: Predictions were not saved inside the loop.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogAA1gJuH1vV"
      },
      "outputs": [],
      "source": [
        "test_path = \"/content/drive/MyDrive/nlp_math_misunderstanding/data/test.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UvbHtLJfKgk"
      },
      "outputs": [],
      "source": [
        "saved_model_path = f\"{output_dir}/fold_5\"\n",
        "test_path = \"/content/drive/MyDrive/nlp_math_misunderstanding/data/test.csv\"\n",
        "model_checkpoint = \"roberta-base\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"--- STARTING INFERENCE PIPELINE ---\")\n",
        "\n",
        "print(\"1. Loading Tokenizer and Model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "base_model.resize_token_embeddings(50293)\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, saved_model_path)\n",
        "model.to(device)\n",
        "\n",
        "trainer = Trainer(model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "print(f\"2. Loading and processing data from {test_path}...\")\n",
        "try:\n",
        "    df_test = pd.read_csv(test_path)\n",
        "    print(f\"   Loaded {len(df_test)} rows.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"   ERROR: File not found. Please check the path.\")\n",
        "    raise\n",
        "\n",
        "df_test['StudentExplanation'] = df_test['StudentExplanation'].fillna(\"\")\n",
        "df_test['QuestionText'] = df_test['QuestionText'].fillna(\"\")\n",
        "df_test['MC_Answer'] = df_test['MC_Answer'].fillna(\"\")\n",
        "\n",
        "df_test['Combined'] = (\n",
        "    df_test['QuestionText'] + \" || \" +\n",
        "    df_test['MC_Answer'] + \" || \" +\n",
        "    df_test['StudentExplanation']\n",
        ")\n",
        "\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"Combined\"], truncation=True, max_length=256)\n",
        "\n",
        "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "columns_to_keep = ['input_ids', 'attention_mask']\n",
        "columns_to_remove = [col for col in tokenized_test.column_names if col not in columns_to_keep]\n",
        "tokenized_test_clean = tokenized_test.remove_columns(columns_to_remove)\n",
        "\n",
        "\n",
        "print(\"3. Running prediction...\")\n",
        "predictions_output = trainer.predict(tokenized_test_clean)\n",
        "y_pred_ids = np.argmax(predictions_output.predictions, axis=1)\n",
        "\n",
        "labels_map = id2label if 'id2label' in locals() else model.config.id2label\n",
        "y_pred_labels = [labels_map[i] for i in y_pred_ids]\n",
        "\n",
        "df_test['Predicted_Misconception'] = y_pred_labels\n",
        "output_filename = \"submission.csv\"\n",
        "df_test.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"DONE! Results saved to '{output_filename}'\")\n",
        "print(df_test[['Combined', 'Predicted_Misconception']].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3PKhySpCfA3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51INqlL4w_1z"
      },
      "outputs": [],
      "source": [
        "path_llm = \"/content/drive/MyDrive/nlp_math_misunderstanding/data/processed/train_v1.csv\"\n",
        "path_no_llm = \"/content/drive/MyDrive/nlp_math_misunderstanding/data/processed/train_v2.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOKgZYfl0vLz"
      },
      "source": [
        "# **Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sample_df = df.sample(2000, random_state=42)\n",
        "tfidf = TfidfVectorizer(max_features=500).fit_transform(sample_df['Combined'])\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embedding = tsne.fit_transform(tfidf.toarray())\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.scatterplot(\n",
        "    x=embedding[:, 0],\n",
        "    y=embedding[:, 1],\n",
        "    hue=sample_df['Misconception'],\n",
        "    legend=False,\n",
        "    palette='tab10',\n",
        "    s=60, alpha=0.7\n",
        ")\n",
        "plt.title('t-SNE Projection of Misconceptions (Semantic Clustering)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jh6AI_-e2w1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "\n",
        "TEST_FILE = f\"{output_dir}/holdout_test_300.csv\"\n",
        "FOLD_TO_TEST = 1\n",
        "MODEL_PATH = f\"{output_dir}/fold_{FOLD_TO_TEST}\"\n",
        "\n",
        "print(f\"Loading Holdout Test Data from: {TEST_FILE}\")\n",
        "df_test = pd.read_csv(TEST_FILE)\n",
        "sample_size = 300\n",
        "df_viz = df_test.sample(sample_size, random_state=42).copy()\n",
        "texts = df_viz['Combined'].tolist()\n",
        "labels = df_viz['label'].tolist()\n",
        "\n",
        "labels_names = [id2label[l] for l in labels]\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Extracting embeddings from RoBERTa...\")\n",
        "with torch.no_grad():\n",
        "    for text in tqdm(texts):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
        "        outputs = model.base_model(**inputs, output_hidden_states=True)\n",
        "\n",
        "        cls_embedding = outputs.hidden_states[-1][0, 0, :].cpu().numpy()\n",
        "        embeddings.append(cls_embedding)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "print(\"Running t-SNE...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "tsne_results = tsne.fit_transform(embeddings)\n",
        "\n",
        "df_viz['x'] = tsne_results[:, 0]\n",
        "df_viz['y'] = tsne_results[:, 1]\n",
        "df_viz['Misconception'] = labels_names\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.scatterplot(\n",
        "    data=df_viz,\n",
        "    x='x', y='y',\n",
        "    hue='Misconception',\n",
        "    palette='tab20',\n",
        "    s=70, alpha=0.8,\n",
        "    legend='full'\n",
        ")\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.title('Semantic Landscape of Misconceptions (RoBERTa Embeddings)', fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pB03iCjWH4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_misconceptions = df['Misconception'].dropna().unique()\n",
        "\n",
        "sorted_misconceptions = sorted(unique_misconceptions)\n",
        "print(f\"{len(sorted_misconceptions)}\")\n",
        "for idx, misconception in enumerate(sorted_misconceptions, 1):\n",
        "    print(f\"{idx}. {misconception}\")"
      ],
      "metadata": {
        "id": "8clrt8k7-XmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvFSRzPJ0SA-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "top_classes = df['Misconception'].value_counts().head(30)\n",
        "sns.barplot(x=top_classes.values, y=top_classes.index, palette='viridis')\n",
        "plt.title('Top 30 Most Frequent Misconceptions (The \"Long Tail\" Problem)')\n",
        "plt.xlabel('Number of Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rf4PMVCh0VnV"
      },
      "outputs": [],
      "source": [
        "df['word_count'] = df['Combined'].apply(lambda x: len(str(x).split()))\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['word_count'], bins=50, kde=True, color='purple')\n",
        "plt.title('Distribution of Text Length (Words per Input)')\n",
        "plt.xlabel('Word Count')\n",
        "plt.xlim(0, 300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJTMBcgk0YGu"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "text = \" \".join(df['Combined'].astype(str).tolist())\n",
        "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('What the Model Sees: Common Terms')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6GtRyyxRxVm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "epochs = [1, 2, 3]\n",
        "val_loss = [0.394353, 0.273984, 0.266884]\n",
        "accuracy = [0.906442, 0.930982, 0.932515]\n",
        "f1_macro = [0.759786, 0.870845, 0.865808]\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "color_loss = 'tab:red'\n",
        "ax1.set_xlabel('Epochs', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Validation Loss', color=color_loss, fontsize=12, fontweight='bold')\n",
        "ax1.plot(epochs, val_loss, marker='o', color=color_loss, linewidth=3, label='Validation Loss')\n",
        "ax1.tick_params(axis='y', labelcolor=color_loss)\n",
        "ax1.set_xticks(epochs)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color_f1 = 'tab:blue'\n",
        "color_acc = 'tab:green'\n",
        "ax2.set_ylabel('Score (F1 / Accuracy)', color='black', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax2.plot(epochs, f1_macro, marker='s', color=color_f1, linewidth=3, label='F1-Macro')\n",
        "ax2.plot(epochs, accuracy, marker='^', color=color_acc, linestyle='--', linewidth=2, label='Accuracy')\n",
        "\n",
        "ax2.tick_params(axis='y', labelcolor='black')\n",
        "ax2.set_ylim(0.7, 1.0)\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right', fontsize=11, frameon=True, shadow=True)\n",
        "\n",
        "plt.title('Training Dynamics (Fold 2): Loss Drop vs Metrics Growth', fontsize=14, pad=15)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig(\"training_dynamics_fold2_manual.png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/nlp_math_misunderstanding/data/raw/train.csv\")\n",
        "df['explanation_len'] = df['StudentExplanation'].astype(str).apply(len)\n",
        "\n",
        "FILTER_LIMIT = 250\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "sns.histplot(df['explanation_len'], bins=100, color='skyblue', label='Всі пояснення')\n",
        "\n",
        "plt.axvline(x=FILTER_LIMIT, color='red', linestyle='--', linewidth=3, label=f'Твій фільтр ({FILTER_LIMIT} символів)')\n",
        "plt.axvspan(xmin=FILTER_LIMIT, xmax=df['explanation_len'].max(), color='red', alpha=0.3, label='Видалені дані (Dropped)')\n",
        "\n",
        "plt.title(f'How much data is removed when filtering: len(StudentExplanation) > {FILTER_LIMIT}', fontsize=14)\n",
        "plt.xlabel('Number of characters in the explanation', fontsize=12)\n",
        "plt.ylabel('Number of examplesв', fontsize=12)\n",
        "plt.xlim(0, 600)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "lost = (df['explanation_len'] > FILTER_LIMIT).sum()\n",
        "total = len(df)\n",
        "print(f\"Total rows: {total}\")\n",
        "print(f\"Will be deleted: {lost} рядків ({lost/total:.2%} від усіх даних)\")"
      ],
      "metadata": {
        "id": "LjSZk4ehWdq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ekx02hNfXKoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}